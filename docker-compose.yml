version: '3.1'
services:
  postgres:
    image: postgres:9.6.5
    environment:
      - POSTGRES_USER=pricewars
      - POSTGRES_PASSWORD=1337
      - POSTGRES_DB=marketplace

  redis:
    image: redis:latest
    ports:
      - "6379:6379"

  zookeeper:
    image: zookeeper:3.4
    ports:
      - "2181:2181"

  kafka:
    image: wurstmeister/kafka:1.1.0
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9092,OUTSIDE://localhost:9093
      KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LOG_DIRS: /kafka/kafka-logs
      KAFKA_LOG_RETENTION_HOURS: -1
      LOG4J_LOGGER_KAFKA: WARN
      LOG4J_LOGGER_ORG_APACHE_ZOOKEEPER: WARN
    depends_on:
      - zookeeper

  kafka-reverse-proxy:
    build: ./kafka-reverse-proxy
    ports:
      - "8001:8001"
    command: ["python3", "-u", "LoggerApp.py", "--kafka_url", "kafka:9092"]
    depends_on:
      - kafka
    links:
      - kafka

  flink-jobmanager:
    image: flink:1.6.0
    ports:
      - "8081:8081"
    expose:
      - "6123"
    environment:
      - KAFKA_URL=kafka:9092
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
    entrypoint: /bin/bash
    command: /analytics/start-jobmanager.sh
    volumes:
      - ./analytics:/analytics:ro
    depends_on:
      - analytics
      - kafka
    links:
      - kafka

  flink-taskmanager:
    image: flink:1.6.0
    expose:
      - "6121"
      - "6122"
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
      - TASK_MANAGER_NUMBER_OF_TASK_SLOTS=8
    # disable terminal output of this container
    logging:
      driver: "none"
    command: taskmanager
    depends_on:
      - flink-jobmanager
    links:
      - "flink-jobmanager:flink-jobmanager"

  analytics:
    build: ./analytics
    # Flink jobs are compiled when this image is created.
    # On container start, the Flink jobs are copied to the host filesystem where they
    # can be found by the flink container.
    # Newer files in the destination are not overwritten by the copy command.
    # This way, the jobs can be updated on the host and used in the docker setup without rebuilding the image.
    command: "bash -c 'mkdir -p /export/target/jars/ && cp -u /analytics/target/jars/* /export/target/jars/ && chmod -R 777 /export/target'"
    volumes:
      - ./analytics:/export

  management-ui:
    build: ./management-ui
    ports:
      - "80:80"
    depends_on:
      - kafka-reverse-proxy
      - marketplace
      - producer

  marketplace:
    build: ./marketplace
    ports:
      - "8080:8080"
    environment:
      - POSTGRES_USER=pricewars
      - POSTGRES_PASSWORD=1337
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=marketplace
      - KAFKA_URL=kafka:9092
      - REDIS_HOST=redis
      - PRICEWARS_PRODUCER_URL=http://producer:3050
    depends_on:
      - postgres
      - redis
      - kafka
      - producer
    links:
      - postgres
      - redis
      - kafka
      - producer

  producer:
    build: ./producer
    ports:
      - "3050:3050"
    environment:
      - KAFKA_URL=kafka:9092
    depends_on:
      - kafka

  consumer:
    build: ./consumer
    ports:
       - "3000:3000"
    environment:
      - RAILS_ENV=development
      - PRICEWARS_MARKETPLACE_URL=http://marketplace:8080
    command: "bash -c 'rm -rf /consumer/tmp/pids/server.pid; bundle exec rails s -b 0.0.0.0'"
    depends_on:
      - marketplace
    links:
      - marketplace

  merchant:
    build: ./merchant
    restart: on-failure:5
    ports:
      - "5003:5003"
    command: python3 -u merchant.py --port 5003 --strategy two_bound --marketplace http://marketplace:8080 --producer http://producer:3050
    volumes:
      - ./merchant:/merchant
    depends_on:
      - producer
      - marketplace
    links:
      - producer
      - marketplace

networks:
  default:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 192.168.47.0/24
